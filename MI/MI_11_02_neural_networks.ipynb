{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs)\n",
    "\n",
    "Inspired by the neural networks found in the brains of humans and animals.\n",
    "These are made up of neurons, which transmit electrical signals.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Components_of_neuron.jpg/1280px-Components_of_neuron.jpg)\n",
    "\n",
    "While the method was inspired by nature, there is a lot we still don't know about how the brain works.\n",
    "ANNs are useful mathematical tools but it's not their purpose to precisely imitate nature.\n",
    "\n",
    "A single artificial neuron (or perceptron) has a similar model as we used for logistic regression.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Antonio-Parmezan/publication/330742498/figure/fig2/AS:721823524200448@1549107547175/Structure-of-Perceptron_W640.jpg)\n",
    "\n",
    "By using multiple neurons, they can be trained simultaneously, for different combinations of the input features.\n",
    "Instead of trying to combine the features ourselves manually, the ANN will learn the relations between the features by itself.\n",
    "By changing the weights, a neuron will be more (or less) sensitive to certain features.\n",
    "\n",
    "The neurons are grouped into layers of the ANN.\n",
    "An input layer, 1 or more hidden (or middle) layers, and an output layer.\n",
    "In the traditional feed-forward (or sequential) networks, the output of one layer is the input of the next layer.\n",
    "(There are other possible structures, we will look at them later.)\n",
    "\n",
    "![](https://miro.medium.com/max/750/1*Uhr-4VDJD0-gnteUNFzZTw.jpeg)\n",
    "\n",
    "By using multiple layers, the ANN can build more and more complex features by combining simple ones.\n",
    "This is shown above on a special type of ANN, a Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass (inference)\n",
    "\n",
    "In most ANNs, every node of a layer is connected to every node of the adjacent layers.\n",
    "These are called dense layers.\n",
    "So, the input vecctor of each neuron in a layer is the same, only their weights can differ.\n",
    "\n",
    "By calculating the weighted sums and the activation functions for each neuron, layer by layer, we can calculate the output of an (already trained) ANN.\n",
    "This is called the forward pass or inference process.\n",
    "\n",
    "In regression models, we've seen how we can calculate the predicted output by dot product of vectors.\n",
    "Here, the input is a 1D vector as well but we have multiple weight vectors - one vector for each neuron of a layer.\n",
    "By combining the weight vectors into a matrix, the dot products for the neurons of one layer can be calculated with a single matrix multiplication.\n",
    "GPUs are very good at it, as 3D graphical transformations are also done by matrix multiplications.\n",
    "\n",
    "But how do we train the network?\n",
    "\n",
    "How do we calculate the error?\n",
    "How do we know which layer causes the error?\n",
    "If we have multiple layers, how do we update their weights?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass (backpropagation)\n",
    "\n",
    "One possible taching algorithm is backpropagation.\n",
    "\n",
    "Essentially, the forward pass calculation is a series of matrix multiplications and function compositions:\n",
    "$$g(x) = f^L(W^L f^{L-1}(W^{L-1} \\dots f^2(W^2 f^1(W^1 x))\\dots))$$\n",
    "\n",
    "The error/cost/loss is the difference between the actual $y$ values in the training set, and the predicted $p=g(x)$ values.\n",
    "This is not new, we can use MSE, MLE, or other loss functions, based on the output type.\n",
    "\n",
    "To find how a particular weight $w$ should be changed, we need the partial derivative $\\frac{\\partial g}{\\partial w}$.\n",
    "\n",
    "In short, we can calculate the derivatives for each layer by going backwards, and using the derivative of a layer to calculate the derivative of the preceding layer.\n",
    "This is also a series of matrix multiplications and elementwise multiplications.\n",
    "\n",
    "For an in-depth explanation of this calculation, read [Chapter 2 of Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html) by Michael Nielsen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Playtime!](http://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow and Keras\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is an open-source ML library created by Google.\n",
    "\n",
    "*Tensor is the generalization of vectors and matrices. A scalar (a number) is a rank-0 tensor, a 1D vector is a rank-1 tensor, a 2D matrix is a rank-2 tensor, etc.*\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level Python API for working with ANNs. It became part of the TensorFlow Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# load dataset containing labeled grayscale images of handwritten digits\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# scale pixel values to 0-1 range\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "print(train_images.shape)  # 60000 28x28 pixel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "count = 10\n",
    "print(\"Index:\", np.array(range(count)))\n",
    "print(\"Label:\", train_labels[:count])\n",
    "px.imshow(train_images[:count], animation_frame=0, color_continuous_scale=\"Greys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.keras.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = tf.keras.Sequential()\n",
    "# the Flatten layer converts the pixel matrix into a 1D vector\n",
    "model.add(tf.keras.layers.Flatten(input_shape=train_images[0].shape))\n",
    "# hidden layer with 128 nodes\n",
    "model.add(tf.keras.layers.Dense(128, \"sigmoid\"))\n",
    "# output layer for the 10 classes\n",
    "model.add(tf.keras.layers.Dense(10, \"sigmoid\"))\n",
    "\n",
    "# shorter version\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=train_images[0].shape),\n",
    "        tf.keras.layers.Dense(128, \"sigmoid\"),\n",
    "        tf.keras.layers.Dense(10, \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    # loss function for classification with numerical labels\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # display accuracy (correct/all) during training\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(train_images, train_labels, epochs=5)  # epochs is the iteration count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the trained model\n",
    "print(model.predict(test_images[:3]))\n",
    "print(\"Label:\", test_labels[:10])\n",
    "px.bar(model.predict(test_images[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of the model is a vector with 10 numbers. The predicted class is the one with the highest value.\n",
    "count = 30\n",
    "print(\"Predicted:\", np.argmax(model.predict(test_images[:count]), axis=1))\n",
    "print(\"Actual:   \", test_labels[:count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the wrong predictions\n",
    "count = 10\n",
    "predictions = np.argmax(model.predict(test_images), axis=1)\n",
    "failures = []\n",
    "fail_images = []\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred != test_labels[i]:\n",
    "        failures.append([i, pred, test_labels[i]])\n",
    "        fail_images.append(test_images[i])\n",
    "print(\"Accuracy on test set:\", 1 - (len(failures) / len(test_labels)))\n",
    "for i, pred, act in failures[:count]:\n",
    "    print(f\"{i}: predicted {pred} instead of {act}\")\n",
    "fail_images = np.array(fail_images[:count])\n",
    "px.imshow(fail_images, animation_frame=0, color_continuous_scale=\"Greys\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, instead of the sigmoid function, a different activation function is preferred.\n",
    "\n",
    "The ReLU (Rectified Linear Unit) performed better in most cases, and easier to calculate:\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "It is not differantiable at 0 but the derivative can be chosen to be either 0 or 1 (as it is $0$ for $x<0$, and $1$ for $x>0$).\n",
    "\n",
    "There are some modified variants to it too:\n",
    "\n",
    "![Source: https://en.wikipedia.org/wiki/ReLU](https://upload.wikimedia.org/wikipedia/commons/4/42/ReLU_and_GELU.svg)\n",
    "\n",
    "![Source: https://www.researchgate.net/publication/341310767_Machine_Learning_for_Materials_Developments_in_Metals_Additive_Manufacturing](https://i.imgur.com/fVxhXQC.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=train_images[0].shape),\n",
    "        tf.keras.layers.Dense(128, \"relu\"),\n",
    "        tf.keras.layers.Dense(\n",
    "            10, \"sigmoid\"\n",
    "        ),  # we want 0-1 values for the output (for now)\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs 10 numbers between 0 and 1. But these are not real probabilities, as they don't add up to 1. (If this was a labeling problem, this would be an appropriate model.)\n",
    "\n",
    "We could divide each value by the sum, then the sum would be 1. This is a linear normalization of the output.\n",
    "$$y_i \\rightarrow \\frac{y_i}{\\sum_{j=1}^n y_j}$$\n",
    "\n",
    "Instead of this, a non-linear normalization approach is used in most cases, called softmax:\n",
    "$$y_i \\rightarrow \\frac{e^{y_i}}{\\sum_{j=1}^n e^{y_j}}$$\n",
    "\n",
    "This also results in the sum being 1 but its main advantage is when used on a wider range of values, not 0-1 values.\n",
    "\n",
    "So, we can use the softmax function as activation function on the output layer instead of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=train_images[0].shape),\n",
    "        tf.keras.layers.Dense(128, \"relu\"),\n",
    "        tf.keras.layers.Dense(10, \"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the trained model\n",
    "px.bar(model.predict(test_images[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above model works well, the same can be done with a different method, which is more efficient and produces fewer numerical errors.\n",
    "\n",
    "Instead of taking the softmax of the values in the output layer, tell the loss function to use logits instead of probabilities.\n",
    "\n",
    "*Logits are the $z$ values in the sigmoid function $\\frac{1}{1+e^{-z}}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=train_images[0].shape),\n",
    "        tf.keras.layers.Dense(128, \"relu\"),\n",
    "        tf.keras.layers.Dense(\n",
    "            10, \"linear\"\n",
    "        ),  # \"linear\" can be omitted, it's the default\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    # tell the loss function to use logits\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 30\n",
    "print(\"Predicted:\", np.argmax(model.predict(test_images[:count]), axis=1))\n",
    "print(\"Actual:   \", test_labels[:count])\n",
    "# make predictions with the trained model\n",
    "pred = model.predict(test_images[:10])\n",
    "print(pred)\n",
    "probs = tf.nn.softmax(pred)\n",
    "px.bar(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when compiling a model, we can choose from several implemented solvers\n",
    "# (or provide our own by defining a custom subclass)\n",
    "# they can also be configured through parameters (e.g. learning_rate)\n",
    "tf.keras.optimizers.Optimizer.__subclasses__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in [\"Adam\", \"SGD\", \"Nadam\", \"Adamax\", \"Adagrad\"]:\n",
    "    print(5 * \"=\", opt, 5 * \"=\")\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=train_images[0].shape),\n",
    "            tf.keras.layers.Dense(128, \"relu\"),\n",
    "            tf.keras.layers.Dense(10, \"linear\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=opt,  # specify optimizer\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.fit(train_images, train_labels, epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also many [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses) and [layer types](https://www.tensorflow.org/api_docs/python/tf/keras/layers) to choose from. We will look at some of them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model\")  # creates a directory for the model files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model\")\n",
    "count = 20\n",
    "print(\"Predicted:\", np.argmax(model.predict(test_images[:count]), axis=1))\n",
    "print(\"Actual:   \", test_labels[:count])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org) is an open-source ML framework made by Facebook (now Meta), based on the [Torch](http://torch.ch/) library. Torch used an API based on the Lua scripting language, while PyTorch offers a Python and a C++ API.\n",
    "\n",
    "TensorFlow is more popular, especially for image recognition applications, while PyTorch is often used for NLP - Natural Language Procesing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "# Download test data.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the training set is large, training on the whole dataset can be slow\n",
    "# Mini-batch learning uses a small subset of the dataset in each step\n",
    "# (For TF with Keras, batch_size can be passed to model.fit(), defaults to 32)\n",
    "# Another approach is SGD (Stochastic Gradient Descent), which only uses\n",
    "# a single, randomly chosen input example for each training step.\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "# Shuffle training dataset before each epoch, so batches will be different\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, len(test_data))\n",
    "\n",
    "# DataLoaders provide an iterator over the batches\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(5):\n",
    "    print(\"epoch\", epoch)\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Forward pass: Compute predicted y by passing X to the model\n",
    "        y_logits = model(X)\n",
    "        # Compute and print loss\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        if batch % 100 == 0:\n",
    "            curr, size = batch * batch_size, len(training_data)\n",
    "            print(f\"  loss: {loss.item():>7f} [{curr:>5d}/{size:>5d}]\")\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "        # Update weights based on gradients\n",
    "        optimizer.step()\n",
    "    # At the end of each epoch, evaluate the model and print accuracy\n",
    "    for X, y in test_dataloader:\n",
    "        y_logits = model(X)\n",
    "        y_pred = torch.argmax(y_logits, dim=1)\n",
    "        acc = torch.mean((y_pred == y).float()).float()\n",
    "        print(f\"accuracy: {acc:%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the same NN structure\n",
    "# A nicer method is to define a custom subclass for nn.Module\n",
    "# and build the network in the ctor\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n",
    ")\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "# Evaluate\n",
    "for i in range(1000):\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "    pred = model(x)\n",
    "    predicted, actual = pred[0].argmax(0), y\n",
    "    if predicted != actual:\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10213d5d3bfbb90c435c171fdf3f7b3db0669a75aa9809df715e84a75ee4f001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
